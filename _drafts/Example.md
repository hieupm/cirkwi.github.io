---
published: true
title: "TITRE DE L'ARTICLE"
excerpt: "SOUS TITRE DE L'ARTICLE"
toc: true
toc_sticky: true
toc_label: "TITRE TOC"
toc_icon: "microchip"
comments: true
header:
overlay_image: "assets/images/nn1/cover.jpg"
teaser: "assets/images/nn1/cover.jpg"
categories: []
---

# Activer les extraits Latex
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

$$J(w, b)=\frac{1}{m}\sum_{1}^{m}\mathfrak{L}(\hat y^{(i)},y^{(i)})$$

# Images
## Internet
<img src="https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png" alt="" class="center">

## Heberg√© Git
<img src="{{ site.url }}{{ site.baseurl }}/assets/images/docker/VM-containers.png" alt="" class="center" width="500">


# Sources
- <a href="https://towardsdatascience.com/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb" target="_blank">Nombreux articles sur la descente de gradient</a> (TowardsDataScience) 
- <a href="https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome" target="_blank">Deep Learning course</a> (Coursera - Andrew Ng)
