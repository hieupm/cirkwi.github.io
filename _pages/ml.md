---
layout: single
permalink: /ml/
author_profile: true
classes: wide
---

Cette section reprend divers projets, etudes, MOOCs que je mène en machine learning.

## Culture générale des algorithmes
[Segmentation des datasets, métriques de performance](https://alexpeterbec.github.io/metrics/scoring/algorithm-scoring/)

## Tensorflow
[De zéro à Tensorflow](https://alexpeterbec.github.io/definitions/tensorflow/tensors/tensorflow-theorie/) : Principes de base de Tensorflow 1.13

[Réseau ConvNet en Tensorflow](https://alexpeterbec.github.io/assets/tensorflow/ConvNet_MNIST.html) : Classification de chiffres manuscrits de la base MNIST avec un réseau ConvNet.

![image](/assets/images/banners/deep-learning1.jpg){: .align-center }

## Concepts du deep learning, descente de gradient

[Qu'est-ce qu'un réseau de neurones ?](https://alexpeterbec.github.io/nn/nn-intro-dl/) : Principe d'un neurone, fonctions associées, exemple pour la classification binaire.

[Descente de gradient](https://alexpeterbec.github.io/ml/graph/algebre/nn-gradient-computation/) : Principe de la descente de gradient, graphes de calcul, application à la regression logistique.

[Python : La vectorisation](https://alexpeterbec.github.io/ml/python/nn-vectorization/) : Comment traiter simultanément des blocs de données grâce au calcul matriciel.

## Réseaux de neurones à une couche cachée

[Complexification du réseau](https://alexpeterbec.github.io/neural/nets/shallow-network/) :

- Passage à une couche cachée
- Adaptation de la descente de gradient

[Grandeurs importantes](https://alexpeterbec.github.io/deep/learning/activation/shallow-activation-functions/) :

- Les fonctions d'activation courantes
- Initialisation des poids pour garantir l'apprentissage

## Réseaux de neurones profonds

[Assemblage de couches](https://alexpeterbec.github.io/deep-learning/deep-networks/) : Notations, forward prop, dimensions des matrices.

[Deep Learning](https://alexpeterbec.github.io/deep-learning/deep-representation/) : Intérêt des réseaux profonds, schemas, Hyper-paramètres.

![image](/assets/images/banners/deep-learning2.jpg){: .align-center }

## Aspects pratiques du deep learning

[Les étapes incontournables pour concevoir un réseau :](https://alexpeterbec.github.io/deep-learning/opt-begin/) 

- Approche itérative pour la conception des réseaux
- Régularisation du modèle : L1, L2, DropOut
- Premiers traitements : Normalisation, explosion du gradient, Initialisation des poids

## Algorithmes d'optimisation

[Méthodes d'optimisation de l'apprentissage :](https://alexpeterbec.github.io/deep-learning/opt-methods/) 

- Utiliser des mini-batches de données
- Optimisation de la direction du gradient (EWMA, RMSprop, Adam)
- Variation du pas d'apprentissage

## Tuning des paramètres, Batch normalisation, Tensorflow

Tuning des hyper-paramètres

Batch-normalisation

Classification multiclasse - Softmax

Introduction à Tensorflow


![image](/assets/images/banners/deep-learning3.jpg){: .align-center }

## Première partie

Orthogonalisation, Métriques d'évaluation, Répartition train/test, Performances

## Seconde partie

Nettoyage des données, Transfer learning, Multi-task learning, end-to-end deep learning

![image](/assets/images/banners/deep-learning4.jpg){: .align-center }

## Les bases des CNN

## Etudes de cas

## Detection d'objets

## Reconnaissance de visages & Neural style transfer

![image](/assets/images/banners/deep-learning5.jpg){: .align-center }

## Réseaux récurrents

## NLP & Word-embedding

## Mécanismes d'attention
