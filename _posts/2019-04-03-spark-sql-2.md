---
published: true
title: "Spark SQL - 2: Querying data"
excerpt: "Getting started with Dataframe and Dataset"
toc: true
toc_sticky: true
toc_label: "Dataframe and Dataset"
toc_icon: "terminal"
author_profile: false
comments: true
header:
  overlay_image: "assets/images/covers/cover-cloud.jpg"
  overlay_filter: 0.2
  teaser: "assets/images/covers/cover-cloud.jpg"
categories: [spark, sql, scala]
---

# Not available handling
[Documentation](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions)

```java
spark.read.json("").na.drop("all")
```
If how is "any", then drop rows containing any null or NaN values. If how is "all", then drop rows only if every column is null or NaN for that row.

```java
spark.read.json("").na.fill("Unknown", Seq("Description"))
```
Take the empty value in the description column and fill it with "Unknown" string value. 

Another version: 
```java
spark.read.json("").na.fill(Map("Description" -> "Unknown"))
```
Or just: 
```java
spark.read.json("").na.fill("Unknown")
```
Every column matching that data type would be checked and replaced. Only numeric, string and boolean types are allowed. 

We need to pay attention about na.replace function. This is a little bit odd in that it doesn't really deal with na values at all but it acts as a way to replace values in specified columns using a found value to replacement value mapping. 

```java
spark.read.json("").na.replace(Seq("Description"),
                        Map("Movies" -> "Entertainment",
                            "Grocery Store" -> "Food"))
```

# A way to check if column exists
```java
implicit class DataFrameHelper(df: Dataframe){
    import scala.util.Try //org.apache.spark.sql.AnalysisException
    def hasColumn(colName: String) = Try(df(colName)).isSuccess
}
```

```java
if(checkDf.hasColumn("_corrupt_record")){
    ...
}
```

# Two ways for concating

```java
df.select(concat($"col_1", lit("-"), $"col_2"))
```

```java
df.select(concat_ws("-", $"col_1", $"col_2"))
```

# "Select distinct"

```java
// #1
df.distinct

// #2
df.dropDuplicate()

// #3
df.dropDuplicate(Seq("col_1", "col_2", "col_3",...))
```

# "Group by" by exemple
[Documentation](https://spark.apache.org/docs/2.4.0/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)


```java
financesDf
    .select($"Account.number".as("AccNum"), $"Amount", $"Description", $"Date")
    .groupBy($"AccNum") // RelationalGroupedDataset
    .agg(avg($"Amount").as("AverageTransaction"),
        sum($"Amount").as("TotalTransaction"),
        min($"Amount").as("NumberOfTransaction"),
        max($"Amount").as("NumberOfTransaction"),
        stddev($"Amount").as("NumberOfTransaction"),
        collect_set($"Description").as("UniqueTransactionDescription"),
        collect_list($"Description").as("ListTransactionDescription"))
```

```java
df.select($"col_1", 
    array_contains($"Descriptions_array","Movies").as("Check"),
    size($"Descriptions_array").as("NumberOfDescriptions")
    sort_array($"Description_array", asc=false).as("SortedDescriptions"))
```

# Flattening Data with Explode

In this section, we will see how we can create a new Dataframe made up a row for every one of the internal array rows, essentially exploding them out to the main dataframe, fusing outer with inner. 

```java
val jsonCompanies = List(
    """{"company":"NewCo", "employees":[{"firstName":"Justin","lastName":"Pihony"},{"firstName":"Jane","lastName":"Doe"}]}""",
    """{"company":"FamilyCo", "employees":[{"firstName":"Rigel","lastName":"Pihony"},{"firstName":"Rory","lastName":"Pihony"}]}""",
    """{"company":"OldCo", "employees":[{"firstName":"Mary","lastName":"Pihony"},{"firstName":"Joe","lastName":"Bob"},null]}""")

val companiesRDD = spark.sparkContext.makeRDD(jsonCompanies)

val companiesDF = spark.read.json(companiesRDD)

companiesDF.show(false)
```

```java
+--------+---------------------------------+
|company |employees                        |
+--------+---------------------------------+
|NewCo   |[[Justin, Pihony], [Jane, Doe]]  |
|FamilyCo|[[Rigel, Pihony], [Rory, Pihony]]|
|OldCo   |[[Mary, Pihony], [Joe, Bob],null]|
+--------+---------------------------------+
```

```java
companiesDF.printSchema
```
```
root
 |-- company: string (nullable = true)
 |-- employees: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- firstName: string (nullable = true)
 |    |    |-- lastName: string (nullable = true)
```
```java
val employeesDfTemp = companiesDF.select($"company", 
                            explode($"employees").as("employee"))
employeesDfTemp.show(false)
```
```java
+--------+----------------+
|company |employee        |
+--------+----------------+
|NewCo   |[Justin, Pihony]|
|NewCo   |[Jane, Doe]     |
|FamilyCo|[Rigel, Pihony] |
|FamilyCo|[Rory, Pihony]  |
|OldCo   |[Mary, Pihony]  |
|OldCo   |[Joe, Bob]      |
+--------+----------------+
```
By default, explode will drop rows that have a null value, spark 2.2 will introduce explode_outer to keep the null value. 

```java
val employeesDfTemp = companiesDF.select($"company", 
                            explode_outer($"employees").as("employee"))
employeesDfTemp.show(false)
```
```java
+--------+----------------+
|company |employee        |
+--------+----------------+
|NewCo   |[Justin, Pihony]|
|NewCo   |[Jane, Doe]     |
|FamilyCo|[Rigel, Pihony] |
|FamilyCo|[Rory, Pihony]  |
|OldCo   |[Mary, Pihony]  |
|OldCo   |[Joe, Bob]      |
|OldCo   |null            |
+--------+----------------+
```
```java
val employeeDF = employeesDfTemp
                    .select($"company", expr("employee.firstName as firstName"))
employeeDF.select($"*", 
                when($"company"==="FamilyCo", "Premium")
                .when($"company"==="OldCo", "Legacy")
                .otherwise("Standard").as("Class")).show(false)

+--------+---------+--------+
|company |firstName|Class   |
+--------+---------+--------+
|NewCo   |Justin   |Standard|
|NewCo   |Jane     |Standard|
|FamilyCo|Rigel    |Premium |
|FamilyCo|Rory     |Premium |
|OldCo   |Mary     |Legacy  |
|OldCo   |Joe      |Legacy  |
|OldCo   |null     |Legacy  |
+--------+---------+--------+
```

From Spark 2.2, we have posexplode function which provides the position of each exploded row. 

```java
val employeesDfTemp = companiesDF.select($"company", posexplode($"employees").as(Seq("employeePosition","employee")))
employeesDfTemp.show(false)

+--------+----------------+----------------+
|company |employeePosition|employee        |
+--------+----------------+----------------+
|NewCo   |0               |[Justin, Pihony]|
|NewCo   |1               |[Jane, Doe]     |
|FamilyCo|0               |[Rigel, Pihony] |
|FamilyCo|1               |[Rory, Pihony]  |
|OldCo   |0               |[Mary, Pihony]  |
|OldCo   |1               |[Joe, Bob]      |
|OldCo   |2               |null            |
+--------+----------------+----------------+
```

# Working with Window function

Firstly, we need to import the window object. 

```java
import org.apache.spark.sql.function._
import org.apache.spark.sql.expression.Window
```

```java
val accountNumberPrevious4WindowSpec = Window.partitionBy($"AccountNumber")
    .orderBy($"Date").rowsBetween(-4,0)
val rollingAvgForPrevious4PerAccount = avg($"Amount").over(accountNumberPrevious4WindowSpec)

financesDf
    .selectExpr("Account.Number as AccountNumber", "Amount",
            "to_date(CAST(unix_timestamp(Date,'MM/dd/yyyy') AS TIMESTAMP)) AS Date")
    .withColumn("RollingAverage",rollingAvgForPrevious4PerAccount)
```

There are two methods you can use to specify your slide: rowsBetween and *rangeBetween*. They accomplish functionnally the same thing, however, rangeBetween is based on the values which is output from the orderBy but not the columns position like rowsBetween. Some special values we can fill out rowsBetween and rangeBetween functions: 

```java
Long.MinValue = Window.unboundedPreceding
Long.MaxValue = Window.unboundedFollowing
0 = Window.currentRow
```

Go further: 
- Standard Functions for Window Aggregation 
  [Documentation](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions)

# User Defined Function

```java
spark.udf.
    register("capitalizeFirstUsingSpace",
        (fullString: String) => fullString.split(" ").map(_.capitalize).mkString(" "))
sampleDF.select($"id", 
                callUDF("capitalizeFirstUsingSpace", $"text").as("text")).show
```

```java
val capitalizerUDF = udf ((fullString: String, splitter: String) 
                => fullString.split(splitter).map(_capitalize).mkString(slippter))
sampleDF.select($"id", capitalizerUDF($"text", lit(" ")).as("text")).show()
```