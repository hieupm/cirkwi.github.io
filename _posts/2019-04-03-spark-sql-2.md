---
published: true
title: "Spark SQL - 2: Querying data"
excerpt: "Getting started with Dataframe and Dataset"
toc: true
toc_sticky: true
toc_label: "Dataframe and Dataset"
toc_icon: "terminal"
author_profile: false
comments: true
header:
  overlay_image: "assets/images/covers/cover-cloud.jpg"
  overlay_filter: 0.2
  teaser: "assets/images/covers/cover-cloud.jpg"
categories: [spark, sql, scala]
---

# Not available handling
Ref: https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions

```java
spark.read.json("").na.drop("all")
```
If how is "any", then drop rows containing any null or NaN values. If how is "all", then drop rows only if every column is null or NaN for that row.

```java
spark.read.json("").na.fill("Unknown", Seq("Description"))
```
Take the empty value in the description column and fill it with "Unknown" string value. 

Another version: 
```java
spark.read.json("").na.fill(Map("Description" -> "Unknown"))
```
Or just: 
```java
spark.read.json("").na.fill("Unknown")
```
Every column matching that data type would be checked and replaced. Only numeric, string and boolean types are allowed. 

We need to pay attention about na.replace function. This is a little bit odd in that it doesn't really deal with na values at all but it acts as a way to replace values in specified columns using a found value to replacement value mapping. 

```java
spark.read.json("").na.replace(Seq("Description"),
                        Map("Movies" -> "Entertainment",
                            "Grocery Store" -> "Food"))
```

# "Where" conditions