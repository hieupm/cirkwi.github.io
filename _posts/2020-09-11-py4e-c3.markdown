---
layout: post
title: Using Python to Access Web Data
date: 2020-09-11 2:32:20 +0200
description:  # Add post description (optional)
fig-caption: # Add figcaption (optional)
tags: [python, coursera, course]
---
This article leverages different use-cases in the courses of **Python For Everyone Specialization (Coursera)** - showing how to treat the Internet as a source of data.  We will scrape, parse, and read web data as well as access data using web APIs.  

We will work with HTML, XML, and JSON data formats in Python. These topics include variables and expressions, conditional execution (loops, branching, and try/except), functions, Python data structures (strings, lists, dictionaries, and tuples), and manipulating files.

## Use-case 1: Extracting Data from JSON
In this use-case, we'll write a Python program prompting for an URL, reading the JSON data from that URL using *urllib* and then parsing and extracting the comment counts from the JSON data, computing the sum of the numbers in the file. 

- Data set 1: http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553)
- Data set 2: http://py4e-data.dr-chuck.net/comments_929141.json

### Data format
The data consists of a number of names and comment counts in JSON as follows:
```json
{
  comments: [
    {
      name: "Matthias"
      count: 97
    },
    {
      name: "Geomer"
      count: 97
    }
    ...
  ]
}
```

### Solution
```python
import json
import urllib.request, urllib.parse, urllib.error
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter location: ')
print('Retrieving', url)
handle = urllib.request.urlopen(url, context=ctx)

data = handle.read()
print('Retrieved', len(data), 'characters')

info = json.loads(data)
print(info)
comments = info['comments']
print('Comment count:', len(comments))

sum = 0
for comment in comments:
    sum += int(comment['count'])

print('Sum:', sum)
```

### Sample Execution
```bash
$ python3 solution.py
Enter location: http://py4e-data.dr-chuck.net/comments_42.json
Retrieving http://py4e-data.dr-chuck.net/comments_42.json
Retrieved 2733 characters
Count: 50
Sum: 2...
```

## Use-case 2: Calling a JSON API
In this use-case we'll write a Python program somewhat prompting for a location, contacting a web service and retrieving JSON for the web service and parsing that data, and getting the first place_id from the JSON. A place ID is a textual identifier that uniquely identifies a place as within Google Maps.

### API End Points

To complete this use-case, we use this API endpoint that has a static subset of the Google Data:

> http://py4e-data.dr-chuck.net/json?

This API uses the same parameter (address) as the Google API. This API also has no rate limit so you can test as often as you like. If you visit the URL with no parameters, you get "No address..." response.

To call the API, you need to include a "key=" parameter and provide the address that you are requesting as the "address=" parameter that is properly URL encoded using the **urllib.parse.urlencode()**.

Different results can be generated depending on the geojson and json endpoints. 

### Sample Execution
```bash
$ python3 solution.py
Enter location: South Federal University
Retrieving http://...
Retrieved 2433 characters
Place id ChIJy0Uc1Zmym4gR3fmAYmWNuac
```

### Solution
```python
import urllib.request, urllib.parse, urllib.error
import json
import ssl

api_key = False
# If you have a Google Places API key, enter it here
# api_key = 'AIzaSy___IDByT70'
# https://developers.google.com/maps/documentation/geocoding/intro

api_key = 42
serviceurl = 'http://py4e-data.dr-chuck.net/json?'

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

while True:
    address = input('Enter location: ')
    if len(address) < 1: break

    parms = dict()
    parms['address'] = address
    parms['key'] = api_key
    url = serviceurl + urllib.parse.urlencode(parms)

    print('Retrieving', url)
    uh = urllib.request.urlopen(url, context=ctx)
    data = uh.read().decode()
    print('Retrieved', len(data), 'characters')

    try:
        js = json.loads(data)
    except:
        js = None

    if not js or 'status' not in js or js['status'] != 'OK':
        print('==== Failure To Retrieve ====')
        print(data)
        continue

    #print(json.dumps(js, indent=4))

    place_id = js['results'][0]['place_id']
    print('Place id', place_id)
```

## Use-case 3: Extracting Data from XML
In this use-case we'll write a Python prompting for a URL, reading the XML data from that URL by using urllib and then parsing and extracting the comment counts from the XML data, computing the sum of the numbers in the file.

- Data set 1: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)
- Data set 2: http://py4e-data.dr-chuck.net/comments_929140.xml (Sum ends with 69)

### Data Format and Approach
The data consists of a number of names and comment counts in XML as follows:

```xml
<comment>
  <name>Matthias</name>
  <count>97</count>
</comment>
```

You are to look through all the '\<comment>' tags and find the '\<count>' values sum the numbers. To make the code a little simpler, we use an XPath selector string to look through the entire tree of XML for any tag named 'count' with the following line of code:

> counts = tree.findall('.//count')

Take a look at the Python ElementTree documentation and look for the supported XPath syntax for details, you could also work from the top of the XML down to the comments node and then loop through the child nodes of the comments node.

### Sample execution
```bash
$ python3 solution.py
Enter location: http://py4e-data.dr-chuck.net/comments_42.xml
Retrieving http://py4e-data.dr-chuck.net/comments_42.xml
Retrieved 4189 characters
Count: 50
Sum: 2...
```

### Solution
```python
import urllib.request, urllib.parse, urllib.error
import xml.etree.ElementTree as ET
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter location: ')
handle = urllib.request.urlopen(url, context=ctx)

data = handle.read()
print('Retrieved', len(data), 'characters')
print(data.decode())
tree = ET.fromstring(data)
counts = tree.findall('.//count')

sum = 0
print('Count: ', len(counts))
for count in counts:
    sum += int(count.text)

print('Sum:', sum)
```

## Use-case 4: Scraping Numbers from HTML using BeautifulSoup
The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.

- http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)
- http://py4e-data.dr-chuck.net/comments_929138.html (Sum ends with 6)

### Data Format
The file is a table of names and comment counts. You can ignore most of the data in the file except for lines like the following:

```html
<tr><td>Modu</td><td><span class="comments">90</span></td></tr>
<tr><td>Kenzie</td><td><span class="comments">88</span></td></tr>
<tr><td>Hubert</td><td><span class="comments">87</span></td></tr>
```

You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers.

```bash
$ python3 solution.py
Enter - http://py4e-data.dr-chuck.net/comments_42.html
Count 50
Sum 2...
```

### Solution

```python
# To run this, download the BeautifulSoup zip file
# http://www.py4e.com/code3/bs4.zip
# and unzip it in the same directory as this file

from urllib.request import urlopen
from bs4 import BeautifulSoup
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter - ')
html = urlopen(url.rstrip(), context=ctx).read()
soup = BeautifulSoup(html, "html.parser")

# Retrieve all of the anchor tags
sum = 0
tags = soup('span')
for tag in tags:
    # Look at the parts of a tag
    sum += int(tag.contents[0])

print("Sum: ", sum)
```

## Use-case 5: Following Links in Python
The program will use urllib to read the HTML from the data files below, extract the *"href="* vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.

- Data set 1: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html
  - Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. 
  - The answer is the last name that you retrieve.
    - Sequence of names: Fikret Montgomery Mhairade Butchi Anayah 
    - Last name in sequence: Anayah
- Data set 2: Start at http://py4e-data.dr-chuck.net/known_by_Harman.html 
  - Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.

### Sample execution

Here is a sample execution of a solution:
```bash
$ python3 solution.py
Enter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html
Enter count: 4
Enter position: 3
Retrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html
Retrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html
Retrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html
Retrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html
Retrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html
```

### Solution
```python
# To run this, download the BeautifulSoup zip file
# http://www.py4e.com/code3/bs4.zip
# and unzip it in the same directory as this file

import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup
import ssl
import re

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter URL: ')

countInput = int(input('Enter count: '))
positionInput = int(input('Enter position: '))

# Get the Following Link by given url and following link position
def getFollowingLink(url, position):
    html = urllib.request.urlopen(url, context=ctx).read()
    soup = BeautifulSoup(html, 'html.parser')
    tags = soup('a')
    link = ''
    try: 
        link = tags[position-1].get('href', None)
        print('Retrieving:', link)
    except:
        print('IndexError: list index out of range')
        quit()
    return link

# Retrieve all of the anchor tags
followingUrl = url
names = list()
nameRegex = r'\S*known_by_([a-zA-Z]+)\S+'

count = 0
print('Retrieving:', followingUrl)
while True: 
    names.append(re.findall(nameRegex, followingUrl)[0])
    if count < countInput:
        followingUrl = getFollowingLink(followingUrl, positionInput)
        count += 1
    else: break

print(names)
```

